5722018448	The events since September 11th have proved my view to be right, and the agencies to be wrong. Al-Qaida's communications security was fit for purpose, in that their attack took the agencies completely by surprise. It did not involve encryption. It did involve hiding messages - among the zillions of innocuous emails that pass across the Internet each day. Two separate FBI reports stated that the hijackers used throwaway webmail accounts, accessed from public libraries.It is disgraceful (but predictable) that the agencies are using the tragedy for bureaucratic empire building, by reviving all the old policies of crypto control that everyone thought dead and buried. If legislators allow themselves to be panicked into giving them their way, the agencies will need huge budget increases, and acquire the ability to push lots more people around. It is much less clear that such policies will do anything of real value in the fight against al-Qaida. Agencies that react to visible policy failure by demanding that their old, failed policies be supported by even more money, and even more coercive power, should be disbanded and replaced with new organisations managed by new people.There has been much press comment on poor airport security, and a lot of effort is going into `tightening' it. Of course, it was clear from the start to security professionals that this was solving the wrong problem. Preventing similar hijackings in the future is a matter for aircraft operating procedures, and the problem is now probably fixed - regardless of anything that governments do.  Thanks to the huge publicity given to the attacks, it will be a long time before a pilot opens the cockpit door and rushes out, just because he's heard that one passenger is disembowelling another.In the USA, the government is taking over responsibility for airport security. This may not be an entirely good thing. The standard government-security model works via perimeter controls: people have clearances, objects have classifications and places are in zones.These are related by a set of access rules and perimeter control (such as no Secret document may be left on a desk overnight' and `only an officer of rank colonel or above can declassify a Top Secret document'). This approach gets used in a rough and ready way in airports (`only staff with a red badge, and ticketed passengers who have gone through metal detectors, can go airside'), but implementing it any more thoroughly is likely to be problematic.In a large airport, over ten thousand employees of over a thousand companies might have airside badges. Many of these have tools that can be used as weapons; many others are minimum-wage staff with at best cursory background checks. How do you stop a casual restaurant cleaner from stealing a chisel from a maintenance carpenter?The standard government-security approach would be to divide the airport into a number of zones, in which there would be clear restrictions on personnel and objects, and between which there would be screening. I expect this would involve a large investment in redesigning the buildings. An alternative would be to use only cleared staff airside, or to limit and subdivide the airside zone by screening passengers and hand baggage at the departure gate - but both of these would also cost more. There is by now a large literature on the practical problems of clearances, managing security compartments and so on; I discuss some of the issues in chapters 7-8 of my book.A number of the points he makes, such as that airports go for visible protective measures (such as bullying customers) rather than effective ones (such as positive matching of bags to passengers at the loading gate) have long been obvious to people in the `trade'. But why does so much money get wasted?Many security systems fail for reasons that are more economic than technical. The people responsible for protecting a system are not usually the people who get hurt when the protection fails. Often this is because the system has changed subtly since it was first designed.During the 1980s, banks connected up their ATM systems into networks, and now instead of just preventing fraud by customers their security people have to worry about lawsuits from other banks. This makes everyone defensive and leads to customers who report a fraud being told they must be mistaken or lying. This in turn makes bank staff realise that they can swindle the system without getting caught, which in turn causes fraud to increase.Often, security should not just be discussed in the language of ciphers, seals and biometrics, but of asymmetric information, adverse selection and moral hazard - in short, of microeconomics. I have written a paper on these issues, which are one of our current research topics. They are also discussed at length in my book.Things also go wrong within organisations when tensions arise between the interests of staff and those of their employers. This leads, for example, for risk reduction measures being turned into due diligence routines: `so long as I do X, Y and Z, I won't get fired'. There are also adverse selection effects; a notorious example is that individuals who are particularly risk-averse often seek jobs in the public sector. The result, as economists have long known, is that most organizations (and especially public sector ones) are excessively cautious; they take many fewer risks than a rational economic agent would in similar circumstances.  There's a nice article on this by John Adams, and an analysis on how the `tightening' of security in the USA following the TWA crash a few years ago probably cost about 60 lives a year by driving airline passengers to drive instead.Although it might seem strange for a security engineer to say this, our societies spend far too much on `security'. Increasing this still further in knee-jerk response to terrorist incidents will hinder economic growth - and is letting the other side dictate the game. The response to terrorist incidents must include much better risk management: better information on the threats and on what the available countermeasures can actually achieve, getting the mathematics of the risk models right, diverting protection resources from show to substance and from due diligence to risk reduction, and - finally - educating the press and public about what can realistically be done, and what can't.The USA may well not find a simple military solution to the Islamic fundamentalism problem, any more than Spain could find a lasting military solution to the Barbary pirate problem in 1504. But, as an academic, I do believe we know in principle how to fix those secondary problems that arise from ignorance. That's after all why I wrote my book.Although there are decent books on some security tools, such as burglar alarms, access controls and cryptography, there has been almost nothing on how to use them in real systems. But most security systems don't fail because the protection mechanisms were weak, but because the designers protected the wrong thing, or protected the right thing in the wrong way. If an extra one or two percent of gross world product is going to be spent on security engineering over the next few years, that amounts to absolutely colossal waste. Reading my book can help you avoid some of it.Engineering - a Guide to Building Dependable Distributed Systems' gives a fairly detailed tutorial on a number of security applications, such as automatic teller machines, burglar alarms, copyright protection mechanisms and electronic warfare systems. It uses these to introduce a wide range of security technologies, such as biometrics, tamper-resistant electronics and authentication protocols.  This material is then used to bring out the system-level engineering issues, such as false alarm rates, protection versus resilience, naming, security usability, reliability, and assurance.A shocking example is an export control bill currently before Britain's parliament. This will enable Tony Blair's government to impose licensing restrictions on collaborations between scientists in the UK and elsewhere; to take powers to review and suppress scientific papers prior to publication; and even to license foreign students taught by British university teachers.During the late 1990s, arms export regulations prevented US nationals making cryptographic software available on their web pages, or sending it abroad by email. Phil Zimmermann, the author of the popular PGP encryption program, was investigated by a Grand Jury for letting it `escape' to the Internet. The law was ridiculed by students wearing T-shirts printed with encryption source code (`Warning - this T-shirt is a munition!') and challenged in the courts as an affront to free speech. Meanwhile, European engineers made crypto software freely available.The Clinton administration fought back, with Al Gore pushing European governments to fall in line. After Tony Blair was elected in 1997, the British government became eager to help, but Parliament was by their first attempt in 1998 to impose export controls on intangibles. They then tried an `end run' around Parliament by quietly negotiating a Europe-wide agreement which they now say we have no choice about implementing.Individual European countries have a lot of latitude about how they implement this agreement, but the British approach is draconian.  The proposed law will give ministers wide powers to regulate the transfer of technologies that could have harmful effects. Ministers  admitted in parliament that their overriding concern was to leave no loopholes: no T-shirts, no bar codes, no faxes, no covert channel through which controlled information could lawfully leave the country.  The law even allows the government to control `non-documentary transfers' (read: speaking to foreigners) in cases where the technology may be used for certain types of weapons, such as guided missiles. As I am currently sitting in an office at MIT, on sabb